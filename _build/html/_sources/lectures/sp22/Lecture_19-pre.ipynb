{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c7e6f5",
   "metadata": {},
   "source": [
    "# Lecture 19 (5/9/2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea5e3d",
   "metadata": {},
   "source": [
    "**Announcements**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac90fc6",
   "metadata": {},
   "source": [
    "*Last time we covered:*\n",
    "- Evaluating classification algorithms\n",
    "\n",
    "**Today's agenda:**\n",
    "- Evaluating classification algorithms cont'd (ROC curves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ebecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ba225",
   "metadata": {},
   "source": [
    "# Advanced classifier evaluation: ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160467e9",
   "metadata": {},
   "source": [
    "First, let's load in the `iris` dataset and remind ourselves what it is we're classifying to begin with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b0e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')\n",
    "iris\n",
    "\n",
    "sns.scatterplot(data = iris, x = \"sepal_length\", y = \"sepal_width\", hue = \"species\", alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efad487",
   "metadata": {},
   "source": [
    "Last time, we discussed:\n",
    "- How to fit a $k$-nearest neighbors (k-NN) classification model to this data \n",
    "- How to evaluate its performance on held-out or cross-validated test data\n",
    "\n",
    "Let's quickly review this together by having you code and run the steps above yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae873e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by changing this to a binary classification problem\n",
    "iris['species_binary'] = iris['species'].map({'versicolor': 'versicolor',\n",
    "                                             'virginica': 'non-versicolor',\n",
    "                                             'setosa': 'non-versicolor'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd080280",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "# 1. Select the `sepal_length` and `sepal_width` columns as X values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Select the `species_binary` column we generated above as y values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Use the `train-test-split` function to generate a set of training and test data points \n",
    "    # Use 0.2 for the test portion and set `random_state = 0` to make our results identical\n",
    "    # Don't forget to call `reset_index(drop = True)` for each of your train/test datasets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "# 4. Fit an sklearn `KNeighborsClassifier` instance to this data with 3 neighbors\n",
    "\n",
    "\n",
    "\n",
    "# 5. Evaluate the classifier using accuracy as a baseline metric\n",
    "\n",
    "\n",
    "\n",
    "# 6. Evaluate the classifier with a more sophisticated metric like F1 score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e89af1",
   "metadata": {},
   "source": [
    "## Hard and Soft Classification\n",
    "\n",
    "The $k$-nearest neighbors classifier chooses a predicted label $\\hat{\\lambda}$ for a new set of features $\\theta$ by selecting the *mode* of the labels of the nearest training items. \n",
    "\n",
    "In this way, you can think of the nearest neighbors as essentially \"voting\" for the label of each test item. This produces a single label prediction for each test item. Put another way, all the predictions for our test items in the example above are either `'versicolor'` or `'non-versicolor'`. This sort of model *decision policy* is referred to as **Hard Classification**. \n",
    "\n",
    "**But, it doesn't need to be this way.** We can also represent the \"votes\" of the nearest neighbors as generating a *probability distribution* over $\\hat{\\lambda}$ values. \n",
    "- For example, if there are 3 nearest neighbors for our new item and 2 are 'versicolor' and 1 is 'non-versicolor', we can represent the new item as having a 2/3 chance of being 'versicolor'. \n",
    "- Many of the classification algorithms we'll discuss can assign a *probability* of a particular label for a given test item rather than a strict label assignment.\n",
    "\n",
    "This form of classification is called **Soft Classification**. \n",
    "\n",
    "The `sklearn` `KNeighborsClassifier` class exports a `predict_proba` function that is just like the `predict` function except instead of showing us the *hard predictions* for each test item, it shows us the *soft prediction* probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the original `predict` function\n",
    "knn.predict(X = xtest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e5de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the graded probability prediction\n",
    "knn.predict_proba(X = xtest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d703aa",
   "metadata": {},
   "source": [
    "In the predictions above, the first column is the probability of 'non-versicolor' and the second is the probability of 'versicolor'.\n",
    "\n",
    "This *Soft Classification* allows us to set more flexible decision policies about what kind of label $\\hat{\\lambda}$ we want to assign to a given test item. Using the *mode* of the nearest neighbor labels in k-NN classification sets a **classification threshold** of 50% (for binary classification), but we could choose any threshold we want depending on the problem.\n",
    "- In some cases, we may want to predict a label value whenever *any* of the neighbors have that label (ex. risk of a fatal disease).\n",
    "- Or, maybe we only want to predict a particular label if 90% of the neighbors have that label (ex. setting a high parole). \n",
    "\n",
    "However, the classification threshold we set will effect how often we assign a particular label to new pieces of data.\n",
    "\n",
    "*Why does this matter?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f55d6a4",
   "metadata": {},
   "source": [
    "## Different thresholds change evaluation metrics\n",
    "\n",
    "Critically, our choice of classification threshold will affect our evaluation metrics above in predictable ways.\n",
    "\n",
    "In our `iris` dataset, labeling new data points based on the mode of 3 nearest neighbors sets a threshold of 2/3 for 'versicolor' labels. \n",
    "\n",
    "If we set a *lower* threshold of 1/3: \n",
    "- We will label *more* test items as 'versicolor' since we only need 1 out of 3 neighbors to be 'versicolor'\n",
    "- We will have a **higher true positive rate** (and **lower false negative rate**) because we are more likely to detect all true versicolor items this way\n",
    "- But, we will have a **higher false positive rate** (and **lower true negative rate**) since we will label more things as 'versicolor' that shouldn't be due to our low threshold.\n",
    "\n",
    "If instead we set a very *high* threshold of 3/3:\n",
    "- We will label *fewer* test items as 'versicolor' since we now need 3 out of 3 neighbors to be 'versicolor' before labeling a new item 'versicolor'.\n",
    "- We will have a **lower true positive rate** (and **higher false negative rate**) because we are more likely to pass up on some true versicolor items this way\n",
    "- We will have a **lower false positive rate** (and **higher true negative rate**) since we will label few things as 'versicolor' that shouldn't be due to our high threshold.\n",
    "\n",
    "\n",
    "Let's illustrate this by looking at this pattern in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72794d3c",
   "metadata": {},
   "source": [
    "## ROC curves: expressing accuracy across different thresholds\n",
    "\n",
    "Below, we're going to compute the \"versicolor probability\" in our test set from above for a classifier with 5 nearest neighbors.\n",
    "\n",
    "Then, we'll look at how different *thresholds* for identifying test items as 'versicolor' change our **true positive rate** and **false positive rate** in opposite directions.\n",
    "\n",
    "**Step 1**: use the k-NN `predict_proba` function shown above to get probability values for each test item $p(\\text{'versicolor'})$ rather than hard classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de5e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 5).fit(X = xtrain, y = ytrain)\n",
    "\n",
    "# Here's our soft classification with 5 nearest neighbors (converted to dataframe for easier processing)\n",
    "versicolor_probs = pd.DataFrame(knn.predict_proba(X = xtest),\n",
    "                                columns = [\"non-versicolor_prob\", \"versicolor_prob\"]\n",
    "                               )\n",
    "\n",
    "# Let's add the true values for comparison\n",
    "versicolor_probs['true_label'] = ytest\n",
    "\n",
    "versicolor_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0a5efc",
   "metadata": {},
   "source": [
    "**Step 2**: Now, let's pick a range of *classification thresholds* for 'versicolor' and show how this changes what values we assign to the test items.\n",
    "\n",
    "For $k$-nearest neighbors, the thresholds are intuitive because we can break them out by each possible voting arrangement of our 5 neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87bfca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which test items get labeled 'versicolor' if you only need 1 nearest neighbor to be versicolor?\n",
    "versicolor_probs['k1_label'] = np.where( # np.where is like an \"if-else\" condition for this column value\n",
    "    versicolor_probs['versicolor_prob'] >= 0.2, # condition\n",
    "    'versicolor', # value in all rows where condition above is True\n",
    "    'non-versicolor' # value in all rows where condition above is False\n",
    ")\n",
    "\n",
    "# Which test items get labeled 'versicolor' if you need at least 2 nearest neighbors to be versicolor?\n",
    "versicolor_probs['k2_label'] = np.where(\n",
    "    versicolor_probs['versicolor_prob'] >= 0.4,\n",
    "    'versicolor',\n",
    "    'non-versicolor'\n",
    ")\n",
    "\n",
    "# Which test items get labeled 'versicolor' if you need at least 3 nearest neighbors to be versicolor?\n",
    "versicolor_probs['k3_label'] = np.where(\n",
    "    versicolor_probs['versicolor_prob'] >= 0.6,\n",
    "    'versicolor',\n",
    "    'non-versicolor'\n",
    ")\n",
    "\n",
    "# Which test items get labeled 'versicolor' if you need at least 4 nearest neighbors to be versicolor?\n",
    "versicolor_probs['k4_label'] = np.where(\n",
    "    versicolor_probs['versicolor_prob'] >= 0.8,\n",
    "    'versicolor',\n",
    "    'non-versicolor'\n",
    ")\n",
    "\n",
    "# Which test items get labeled 'versicolor' if you need *all 5* nearest neighbors to be versicolor?\n",
    "versicolor_probs['k5_label'] = np.where(\n",
    "    versicolor_probs['versicolor_prob'] >= 1.0,\n",
    "    'versicolor',\n",
    "    'non-versicolor'\n",
    ")\n",
    "\n",
    "versicolor_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5ccd1b",
   "metadata": {},
   "source": [
    "**Step 3**: How do our **true positive rate** and **false positive rate** change for each of these thresholds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f177728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# What's the TPR for our lowest threshold (k1) above?\n",
    "k1_tpr = recall_score(y_true = versicolor_probs['true_label'], \n",
    "                      y_pred = versicolor_probs['k1_label'],\n",
    "                      labels = ['versicolor', 'non-versicolor'],\n",
    "                      pos_label = 'versicolor'\n",
    "                     )\n",
    "# What's the FPR for our lowest threshold (k1) above?\n",
    "# Note FPR = 1 - TNR and TNR is just the recall (TPR) for the *negative* label, which we can set below\n",
    "k1_fpr = 1 - recall_score(y_true = versicolor_probs['true_label'], \n",
    "                          y_pred = versicolor_probs['k1_label'],\n",
    "                          labels = ['versicolor', 'non-versicolor'],\n",
    "                          pos_label = 'non-versicolor' # positive label\n",
    "                         )\n",
    "\n",
    "# Same as above for k2 threshold\n",
    "k2_tpr = recall_score(y_true = versicolor_probs['true_label'], \n",
    "                      y_pred = versicolor_probs['k2_label'],\n",
    "                      labels = ['versicolor', 'non-versicolor'],\n",
    "                      pos_label = 'versicolor'\n",
    "                     )\n",
    "k2_fpr = 1 - recall_score(y_true = versicolor_probs['true_label'], \n",
    "                          y_pred = versicolor_probs['k2_label'],\n",
    "                          labels = ['versicolor', 'non-versicolor'],\n",
    "                          pos_label = 'non-versicolor'\n",
    "                         )\n",
    "\n",
    "# Same as above for k3 threshold\n",
    "k3_tpr = recall_score(y_true = versicolor_probs['true_label'], \n",
    "                      y_pred = versicolor_probs['k3_label'],\n",
    "                      labels = ['versicolor', 'non-versicolor'],\n",
    "                      pos_label = 'versicolor'\n",
    "                     )\n",
    "k3_fpr = 1 - recall_score(y_true = versicolor_probs['true_label'], \n",
    "                          y_pred = versicolor_probs['k3_label'],\n",
    "                          labels = ['versicolor', 'non-versicolor'],\n",
    "                          pos_label = 'non-versicolor'\n",
    "                         )\n",
    "\n",
    "\n",
    "# Same as above for k4 threshold\n",
    "k4_tpr = recall_score(y_true = versicolor_probs['true_label'], \n",
    "                      y_pred = versicolor_probs['k4_label'],\n",
    "                      labels = ['versicolor', 'non-versicolor'],\n",
    "                      pos_label = 'versicolor'\n",
    "                     )\n",
    "k4_fpr = 1 - recall_score(y_true = versicolor_probs['true_label'], \n",
    "                          y_pred = versicolor_probs['k4_label'],\n",
    "                          labels = ['versicolor', 'non-versicolor'],\n",
    "                          pos_label = 'non-versicolor'\n",
    "                         )\n",
    "\n",
    "# Same as above for k5 threshold\n",
    "k5_tpr = recall_score(y_true = versicolor_probs['true_label'], \n",
    "                      y_pred = versicolor_probs['k5_label'],\n",
    "                      labels = ['versicolor', 'non-versicolor'],\n",
    "                      pos_label = 'versicolor'\n",
    "                     )\n",
    "k5_fpr = 1 - recall_score(y_true = versicolor_probs['true_label'], \n",
    "                          y_pred = versicolor_probs['k5_label'],\n",
    "                          labels = ['versicolor', 'non-versicolor'],\n",
    "                          pos_label = 'non-versicolor'\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b9d1cb",
   "metadata": {},
   "source": [
    "Phew! Now let's combine the above to see how they compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d380bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of our thresholds, what is the true positive rate and the false positive rate?\n",
    "\n",
    "np.arange(0, 1.1, 0.2)\n",
    "roc_df = pd.DataFrame({\n",
    "    \"threshold\": np.arange(0.2, 1.1, 0.2),\n",
    "    \"TPR\": [k1_tpr, k2_tpr, k3_tpr, k4_tpr, k5_tpr],\n",
    "    \"FPR\": [k1_fpr, k2_fpr, k3_fpr, k4_fpr, k5_fpr]\n",
    "})\n",
    "\n",
    "roc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1756da08",
   "metadata": {},
   "source": [
    "Now, our final step is to graph this relationship. \n",
    "\n",
    "This is called an **[ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)** (Receiver Operating Characteristic) and we'll see why it's useful once we've plotted it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42de497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve by hand\n",
    "\n",
    "g = sns.pointplot(\n",
    "    data = roc_df,\n",
    "    x = \"FPR\",\n",
    "    y = \"TPR\"\n",
    ")\n",
    "\n",
    "g.set_xlabel(\"False Positive Rate\")\n",
    "g.set_ylabel(\"True Positive Rate\")\n",
    "g.set_xticklabels([])\n",
    "g.set_yticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e30880",
   "metadata": {},
   "source": [
    "*What is that??*\n",
    "\n",
    "Answer: a somewhat clumsy ROC curve (since we're not working with much data here).\n",
    "\n",
    "*What does this mean?*\n",
    "\n",
    "- Each of our points above is the TPR and FPR for a given *classification threshold*\n",
    "- When we set a *low threshold*, we expect TPR and FPR to be very high (top right)\n",
    "- When we set a *high threshold*, we expect TPR and FPR to both be very low (bottom left)\n",
    "\n",
    "What about in between?\n",
    "\n",
    "- For every threshold in between the top right (low) and bottom left (high), we want to *keep TPR high while FPR goes down*. \n",
    "- Put another way, we want to reduce FPR without having to also sacrifice TPR. \n",
    "\n",
    "Given the above, **a good ROC curve is one that swings as close to the *top left* portion of the axes as possible**.\n",
    "\n",
    "Here's a picture that illustrates this:\n",
    "\n",
    "![roc](img/roc.png)\n",
    "\n",
    "([source](https://medium.com/the-owl/evaluation-metrics-part-3-47c315e07222))\n",
    "\n",
    "\n",
    "**Compared to what?**\n",
    "\n",
    "Note the dashed line across the middle. This indicates what you would expect to happen with TPR and FPR if your classifier was *random*. In other words, we can compare our ROC curve to the \"random classifier\" line to determine how much better our classifier is doing than random guessing. \n",
    "\n",
    "We use a metric called *area under the curve (AUC)* to calculate this difference.\n",
    "\n",
    "This measures the area under the ROC curve. The value ranges from 0 to 1, but since a random classifier has an AUC of 0.5, we're looking for values > 0.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fada00e",
   "metadata": {},
   "source": [
    "## ROC curves in scikit-learn\n",
    "\n",
    "Fortunately, we don't need to do all the manual calculations above to generate an ROC curve.\n",
    "\n",
    "As usual, `scikit-learn` has us covered!\n",
    "\n",
    "Below, we call the `roc_curve` function to generate an ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(\n",
    "    y_true = ytest,\n",
    "    y_score = knn.predict_proba(X = xtest)[:, 1],\n",
    "    pos_label = 'versicolor'\n",
    ")\n",
    "\n",
    "\n",
    "tpr\n",
    "fpr\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f870740c",
   "metadata": {},
   "source": [
    "Then, we can graph the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pointplot(\n",
    "    x = fpr,\n",
    "    y = tpr\n",
    ")\n",
    "\n",
    "g.set_xlabel(\"False Positive Rate\")\n",
    "g.set_ylabel(\"True Positive Rate\")\n",
    "g.set_xticklabels(np.arange(0, 1.1, 0.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04768794",
   "metadata": {},
   "source": [
    "### AUC\n",
    "\n",
    "`scikit-learn` also exports an AUC function that will report how our ROC curve fares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "roc_auc_score(\n",
    "    y_true = ytest,\n",
    "    y_score = knn.predict_proba(X = xtest)[:, 1], # get probabilities for 'versicolor'\n",
    "    labels = ['versicolor', 'non-versicolor']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d5b7cf",
   "metadata": {},
   "source": [
    "# Evaluating classification algorithms: summary\n",
    "\n",
    "\n",
    "When we use a classification algorithm like $k$-nearest neighbors, we want a way to quantify *how well it performs* with test data.\n",
    "\n",
    "1. The most intuitive way to do so is with accuracy, but this can mis-construe our classifier's performance when we have imbalanced data.\n",
    "\n",
    "2. Alternatives that rely on the *confusion matrix* allow us to weight the relative impact of different kinds of *errors* (false positives and false negatives).\n",
    "\n",
    "3. Recognizing that our rate of false positives and false negatives is sensitive to the *classification threshold* used in our model (e.g., the \"voting\" procedure in k-NN), we can use an ROC curve to examine the classifier's success across a range of thresholds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bac7ae",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b074520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dbfdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62327e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dafd813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e774977668b7c0ae8309835a5187aa7fbf7669e7d0bb59755bc63e573643edcd"
  },
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

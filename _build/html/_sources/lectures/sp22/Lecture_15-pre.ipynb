{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c7e6f5",
   "metadata": {},
   "source": [
    "# Lecture 15 (4/29/2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea5e3d",
   "metadata": {},
   "source": [
    "**Announcements**\n",
    "- Pset 4 will be released today, due next Friday 5/6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac90fc6",
   "metadata": {},
   "source": [
    "*Last time we covered:*\n",
    "- Evaluating regression: $R^2$, out-of-sample prediction, parameter interpretation\n",
    "\n",
    "**Today's agenda:**\n",
    "- Polynomial regression, multiple regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ebecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in and prepare the data we were using last time\n",
    "mpg = sns.load_dataset('mpg')\n",
    "mpg_clean = mpg.dropna().reset_index(drop = True)\n",
    "\n",
    "mpg_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3d210",
   "metadata": {},
   "source": [
    "# Review: Evaluating linear regression\n",
    "\n",
    "In last lecture, we talked about three ways of checking that your regression fit the data well. \n",
    "1. $R^2$ *coefficient of determination*\n",
    "2. Out of sample prediction accuracy\n",
    "3. High confidence (and useful) parameter estimates\n",
    "\n",
    "\n",
    "Let's start by running through each of these in a little more detail since we didn't get much time to discuss them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9266f4f5",
   "metadata": {},
   "source": [
    "## $R^2$, the *coefficient of determination*\n",
    "\n",
    "![determination](img/Determination.png)\n",
    "\n",
    "\n",
    "$ R^2 = 1 - \\dfrac{RSS}{TSS} $\n",
    "\n",
    "$ RSS = \\sum_{i=1}^{n}{(y_i - \\hat{y_i})}^2 $\n",
    "\n",
    "$ TSS = \\sum_{i=1}^{n}{(y_i - \\bar{y})}^2 $\n",
    "\n",
    "$R^2$ ranges between 0 and 1 and can be thought of as the *percentage of variance in $y$ that our model explains*.\n",
    "\n",
    "To understand how it works, remember that RSS is 0 when the regression *perfectly predicts our data* and RSS is equal to TSS when we just guess $\\bar{y}$ for every data point $y_i$ (worst case for our regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62327e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scikit-learn LinearRegression class surfaces a function called `score` that computes R^2\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Format values\n",
    "x_vals = np.array(mpg_clean['weight']).reshape(len(mpg_clean['weight']), 1)\n",
    "y_vals = np.array(mpg_clean['horsepower'])\n",
    "\n",
    "# Fit regression\n",
    "mod = LinearRegression().fit(X = x_vals, y = y_vals)\n",
    "\n",
    "rsq_mod = mod.score(X = x_vals, y = y_vals) # R^2 value\n",
    "rsq_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ffd3d1",
   "metadata": {},
   "source": [
    "Last time, we showed how to calculate the $R^2$ value by hand using the LinearRegression `predict` function. \n",
    "\n",
    "If you're feeling hazy on $R^2$, I recommend going back to the code from that lecture and going through the part where we calculate $R^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb699105",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b24466",
   "metadata": {},
   "source": [
    "## Out of sample prediction\n",
    "\n",
    "![train](img/train.jpeg)\n",
    "\n",
    "\n",
    "**Motivation** \n",
    "\n",
    "If our model is the right fit to our data, it should predict other data from the same underlying distribution or generative process pretty well.\n",
    "\n",
    "**How to check this**\n",
    "\n",
    "There are a lot of ways to test out of sample data which we'll get into in more detail on Monday, but the high-level approach is almost always:\n",
    "1. *Randomly* select a subset of your original data (20-25%) and set it aside as *test data*. The remaining data is your *training data*.\n",
    "2. Fit your model to the *training data only*. \n",
    "3. See how well your fitted model predicts the *test data*. Compare it to the predictions on the training data with something like *Mean Squared Error* (MSE). \n",
    "4. Often, repeat steps 1-3 in various ways (more on that later).\n",
    "\n",
    "**Comparing train and test performance**\n",
    "\n",
    "Step 3 above is critical. One common approach is to use *Mean Squared Error* (MSE):\n",
    "\n",
    "$ MSE = \\dfrac{1}{n - 2} \\sum_{i=1}^{n}{(y_i - \\hat{y_i})}^2 = \\dfrac{1}{n - 2} \\sum_{i=1}^{n}{\\epsilon_i}^2 $\n",
    "\n",
    "This tells you, on average, how close your model was to the true value across all the data points (the $n-2$ is specific to linear regression where we have two parameters, $\\beta_0$ and $\\beta_1$, so $n-2$ is our degrees of freedom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb98c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Use the sklearn `train_test_split` to make this easy\n",
    "from sklearn.metrics import mean_squared_error # Use the sklearn `mean_squared_error` for quick MSE calculation\n",
    "\n",
    "# Randomly sample 25% of our data points to be test data\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x_vals, \n",
    "                                                y_vals, \n",
    "                                                test_size = 0.25\n",
    "                                               )\n",
    "# Fit the model on the training data\n",
    "mod_tr = LinearRegression().fit(X = xtrain, y = ytrain)\n",
    "\n",
    "# Generate model predictions for the test data\n",
    "mod_preds_test = mod_tr.predict(X = xtest)\n",
    "\n",
    "# Compare MSE for the model predictions on train and test data\n",
    "mod_preds_train = mod_tr.predict(X = xtrain)\n",
    "\n",
    "mse_train = mean_squared_error(y_true = ytrain, y_pred = mod_preds_train)\n",
    "mse_train # Note this divides by n rather than n-2 but that's not super important for our purposes\n",
    "\n",
    "mse_test = mean_squared_error(y_true = ytest, y_pred = mod_preds_test)\n",
    "mse_test\n",
    "\n",
    "print(\"Training MSE: {} \\nTest MSE: {}\".format(mse_train, mse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f174d",
   "metadata": {},
   "source": [
    "Just for fun, try running the code above several times and look at how different the values are. \n",
    "\n",
    "*More on this next week...*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479ce863",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357855a4",
   "metadata": {},
   "source": [
    "## Parameter estimates\n",
    "\n",
    "![right](img/right.png)\n",
    "\n",
    "The criteria above are mostly concerned with whether we're doing *a good job predicting our $y$ values* with this model.\n",
    "\n",
    "In many cases, part of what we're concerned with isn't just how well we predict our data, but what kind of relationship our model estimates between $x$ and $y$. \n",
    "- How large or small is the slope?\n",
    "- How confident are we in the estimate?\n",
    "\n",
    "To assess this, we typically compute confidence bounds on the parameter estimates (95% confidence interval or standard error) and compare them to a null value of 0 using $t$ tests.\n",
    "\n",
    "**Linear regression parameter estimates are most useful when they are high confidence and significantly different from 0.**\n",
    "\n",
    "The sklearn `LinearRegression` class doesn't include functions for this sort of analysis, but other tools like the `statsmodels` regression class do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Fit the model\n",
    "results = smf.ols('horsepower ~ weight', data = mpg_clean).fit()\n",
    "\n",
    "# View the results\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f32f6a0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc8506d",
   "metadata": {},
   "source": [
    "# Problems with simple linear regression\n",
    "\n",
    "![corr_ex](img/corr_ex.png)\n",
    "Disclaimer: this figure (from wikipedia) shows *correlation* values associated with these datasets, but the limitations of correlation in capturing these patterns holds for linear regression as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882c5313",
   "metadata": {},
   "source": [
    "## Polynomial regression: non-linear relationship between $x$ and $y$\n",
    "\n",
    "Non-linear data can take all kinds of forms, though there are probably a few that are most common.\n",
    "\n",
    "Let's take a look at a simple example from our cars dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be22b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data = mpg_clean, x = \"horsepower\", y = \"mpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6be5ba0",
   "metadata": {},
   "source": [
    "Does this data have a linear relationship between $x$ and $y$? Seems like it might be more complicated.\n",
    "\n",
    "Enter: polynomial regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a65e7c5",
   "metadata": {},
   "source": [
    "### Polynomial regression: overview\n",
    "\n",
    "Polynomial regression is just like linear regression except that instead of fitting a linear function to the data, we fit higher degree polynomials. \n",
    "\n",
    "Previously, our simple linear regression model assumed that our data $(x_i, y_i)$ could be described as:\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$\n",
    "\n",
    "The OLS process estimates values for $\\beta_0$ and $\\beta_1$ that correspond to a straight line that minimizes $\\epsilon_i$.\n",
    "\n",
    "With polynomial regression, we extend this basic model to include functions of the form:\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i$ for *degree 2* polynomial regression,\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\epsilon_i$ for *degree 3* polynomial regression, \n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\ ... \\ + \\beta_n x_i^n + \\epsilon_i$ for *degree n* polynomial regression.\n",
    "\n",
    "Even though this seems much more complex, polynomial regression uses the same *Ordinary Least Squares* (OLS) parameter estimation as simple regression. You can think of simple linear regression as a special case of polynomial regression. \n",
    "\n",
    "**This gives us immense flexibility to fit more complex functions to our data.** Some of the data illustrated at the top of this section can *only* be modeled using more complex polynomials (see example below as well).\n",
    "\n",
    "**CAUTION**: most of the time you *don't* need polynomials to capture your data. Bad things happen when you use them for data that doesn't have an underlying non-linear structure. More on this on Monday.\n",
    "\n",
    "![poly](img/Poly.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6551f44c",
   "metadata": {},
   "source": [
    "### Polynomial regression in python\n",
    "\n",
    "We can use the numpy `polyfit` library to fit 2nd and 3rd order polynomials to this data\n",
    "(Note: this is probably the simplest method, but there's code to use the familiar scikit learn approach as well below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e44a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can fit higher order polynomial functions to our data rather than just a linear function\n",
    "deg1_fits = np.polyfit(mpg_clean.horsepower, mpg_clean.mpg, 1)\n",
    "deg2_fits = np.polyfit(mpg_clean.horsepower, mpg_clean.mpg, 2)\n",
    "deg3_fits = np.polyfit(mpg_clean.horsepower, mpg_clean.mpg, 3)\n",
    "\n",
    "p1 = np.poly1d(deg1_fits)\n",
    "p2 = np.poly1d(deg2_fits)\n",
    "p3 = np.poly1d(deg3_fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7181eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do the functions fitted above predict for our data?\n",
    "\n",
    "preds = mpg_clean.loc[:, ('horsepower', 'mpg')]\n",
    "\n",
    "preds['deg1_pred'] = p1(preds['horsepower'])\n",
    "preds['deg2_pred'] = p2(preds['horsepower'])\n",
    "preds['deg3_pred'] = p3(preds['horsepower'])\n",
    "\n",
    "preds\n",
    "\n",
    "preds_long = preds.melt(\n",
    "    id_vars = ['horsepower', 'mpg']\n",
    ")\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, our original data\n",
    "sns.scatterplot(data = preds_long,\n",
    "                x = 'horsepower',\n",
    "                y = 'mpg',\n",
    "                color = 'm',\n",
    "                alpha = 0.1\n",
    "               )\n",
    "\n",
    "# Now add in our lines\n",
    "sns.lineplot(data = preds_long,\n",
    "             x = 'horsepower',\n",
    "             y = 'value',\n",
    "             hue = 'variable'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d02d8",
   "metadata": {},
   "source": [
    "Here's the solution using scikit learn; it's a bit more complicated, though it does let you keep using the `LinearRegression` class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae7873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x_vals = np.array(mpg_clean['horsepower']).reshape(len(mpg_clean['horsepower']), 1)\n",
    "y_vals = np.array(mpg_clean['mpg'])\n",
    "preds = mpg_clean.loc[:, ('horsepower', 'mpg')]\n",
    "\n",
    "# Simple linear model\n",
    "mod1 = LinearRegression().fit(x_vals, y_vals)\n",
    "\n",
    "# 2nd order polynomial\n",
    "poly2 = PolynomialFeatures(degree = 2, include_bias = False)\n",
    "x2_features = poly2.fit_transform(x_vals)\n",
    "mod2 = LinearRegression().fit(x2_features, y_vals)\n",
    "\n",
    "\n",
    "# 3rd order polynomial\n",
    "poly3 = PolynomialFeatures(degree = 3, include_bias = False)\n",
    "x3_features = poly3.fit_transform(x_vals)\n",
    "mod3 = LinearRegression().fit(x3_features, y_vals)\n",
    "\n",
    "\n",
    "mod2.coef_\n",
    "# mod3.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6225b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions for each model so we can view how it does\n",
    "preds['deg1_pred'] = mod1.predict(x_vals)\n",
    "preds['deg2_pred'] = mod2.predict(x2_features)\n",
    "preds['deg3_pred'] = mod3.predict(x3_features)\n",
    "\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c9438",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_long = preds.melt(\n",
    "    id_vars = ['horsepower', 'mpg']\n",
    ")\n",
    "preds\n",
    "\n",
    "# First, our original data\n",
    "sns.scatterplot(data = preds_long,\n",
    "                x = 'horsepower',\n",
    "                y = 'mpg',\n",
    "                color = 'm',\n",
    "                alpha = 0.1\n",
    "               )\n",
    "\n",
    "# Now add in our lines\n",
    "sns.lineplot(data = preds_long,\n",
    "             x = 'horsepower',\n",
    "             y = 'value',\n",
    "             hue = 'variable'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b179fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the R^2 values for these models to see what kind of improvement we get\n",
    "# (more on this next week)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5078575",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c8a7cb",
   "metadata": {},
   "source": [
    "## Multiple regression: multiple predictors for $y$\n",
    "\n",
    "Another basic scenario that arises when predicting a continuous variable (probably more commonly than polynomial regression) is having *multiple predictors*.\n",
    "\n",
    "Let's take a look at an intuitive example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402638bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap = pd.read_csv(\"https://raw.githubusercontent.com/UCSD-CSS-002/ucsd-css-002.github.io/master/datasets/gapminder.csv\")\n",
    "\n",
    "# Let's keep just some of the variables (note for pset!)\n",
    "gap_subset = gap.loc[gap['year'] == 2007, ('country', 'year', 'lifeExp', 'pop', 'gdpPercap')]\n",
    "\n",
    "# Add log population\n",
    "gap_subset['logPop'] = np.log10(gap_subset['pop'])\n",
    "gap_subset['logGdpPercap'] = np.log10(gap_subset['gdpPercap'])\n",
    "gap_subset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82900af",
   "metadata": {},
   "source": [
    "In the last problem set, you generated a graph that predicted life expectancy as a function of income, with information about population and region available as well. \n",
    "\n",
    "![gap](img/gapminder.png)\n",
    "\n",
    "The graph suggests that life expectancy is strongly predicted by income, while population may not play such an important role. \n",
    "\n",
    "Let's test that here!\n",
    "\n",
    "What that amounts to asking is:\n",
    "\n",
    "**Can we predict life expectancy using both income *and* population better than we could only using one of those variables?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30763dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = gap_subset, \n",
    "                x = \"logGdpPercap\", # x1 \n",
    "                y = \"lifeExp\",\n",
    "                color = \"r\"\n",
    "               )\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(data = gap_subset, \n",
    "                x = \"logPop\", # x2 \n",
    "                y = \"lifeExp\",\n",
    "                color = \"b\"\n",
    "               )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc5a94",
   "metadata": {},
   "source": [
    "### Multiple regression: overview\n",
    "\n",
    "Multiple regression is like linear regression except that we assume our dependent variable $y_i$ is *jointly* predicted by multiple independent variables $x_1$, $x_2$, ..., $x_n$, as in the example above.\n",
    "\n",
    "As noted above, our simple linear regression model assumes that our data $(x_i, y_i)$ has the following form:\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$\n",
    "\n",
    "With multiple regression, we now extend this model to include multiple predictors:\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\ ... \\ + \\beta_n x_{i,n} + \\epsilon_i $\n",
    "\n",
    "In most cases, multiple regression once again uses the same *Ordinary Least Squares* (OLS) parameter estimation as simple regression. However, interpreting the parameter estimates is a little less straightforward.\n",
    "\n",
    "*How would we interpret $\\beta_0 = 1$, $\\beta_1 = 2$, $\\beta_2 = 3$*?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1042aca",
   "metadata": {},
   "source": [
    "### Multiple regression in python\n",
    "\n",
    "To run our multiple regression, we can use the scikit `LinearRegression` class with just a few modifications to our simple regression code.\n",
    "\n",
    "I've also included the statsmodels code below as well so we can look at the statistics more closely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit learn approach\n",
    "x_vals = np.array(gap_subset[['logGdpPercap', 'logPop']]) # Note: this syntax is important!\n",
    "x_vals = x_vals.reshape(len(gap_subset), 2)\n",
    "x_vals\n",
    "\n",
    "y_vals = np.array(gap_subset['lifeExp'])\n",
    "y_vals\n",
    "\n",
    "mod = LinearRegression().fit(X = x_vals, y = y_vals)\n",
    "\n",
    "mod.intercept_\n",
    "mod.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a37e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How well does our regression do?\n",
    "mod.score(X = x_vals, y = y_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bd8ab4",
   "metadata": {},
   "source": [
    "Using the statsmodels regression class, we can view statistical tests on our parameter fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d6ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_reg = smf.ols('lifeExp ~ logGdpPercap + logPop', data = gap_subset).fit()\n",
    "\n",
    "# View the results\n",
    "multiple_reg.summary()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e774977668b7c0ae8309835a5187aa7fbf7669e7d0bb59755bc63e573643edcd"
  },
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

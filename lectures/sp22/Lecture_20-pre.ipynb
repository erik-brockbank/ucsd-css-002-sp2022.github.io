{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c7e6f5",
   "metadata": {},
   "source": [
    "# Lecture 20 (5/11/2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea5e3d",
   "metadata": {},
   "source": [
    "**Announcements**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac90fc6",
   "metadata": {},
   "source": [
    "*Last time we covered:*\n",
    "- ROC curves\n",
    "\n",
    "**Today's agenda:**\n",
    "- Common classification models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ebecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3d210",
   "metadata": {},
   "source": [
    "# Common Classification Models\n",
    "\n",
    "- $k$-nearest neighbors\n",
    "- Logistic regression\n",
    "- Decision trees\n",
    "- Support Vector Machines (SVMs)\n",
    "- Other: naive Bayes, neural networks, discriminant analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4357ce",
   "metadata": {},
   "source": [
    "## Data: Predicting Heart Disease\n",
    "\n",
    "\n",
    "From [source](https://hastie.su.domains/ElemStatLearn/):\n",
    "\n",
    "> A retrospective sample of males in a heart-disease high-risk region\n",
    "of the Western Cape, South Africa. There are roughly two controls per\n",
    "case of CHD. Many of the CHD positive men have undergone blood\n",
    "pressure reduction treatment and other programs to reduce their risk\n",
    "factors after their CHD event. In some cases the measurements were\n",
    "made after these treatments. These data are taken from a larger\n",
    "dataset, described in  Rousseauw et al, 1983, South African Medical\n",
    "Journal. \n",
    "\n",
    "- sbp: systolic blood pressure\n",
    "- tobacco: cumulative tobacco (kg)\n",
    "- ldl: low densiity lipoprotein cholesterol\n",
    "- adiposity\n",
    "- famhist: family history of heart disease (Present, Absent)\n",
    "- typea: type-A behavior\n",
    "- obesity\n",
    "- alcohol: current alcohol consumption\n",
    "- age: age at onset\n",
    "- chd: **response**, coronary heart disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54222ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.data')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a86246",
   "metadata": {},
   "source": [
    "**Setting up our classifiers**:\n",
    "\n",
    "Let's stick to just a single feature (age at onset) and see how different methods use this feature to predict the outcome label (CHD). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = np.array(data['age']).reshape(len(data), 1)\n",
    "y_vals = np.array(data['chd'])\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x_vals, y_vals, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b79ea",
   "metadata": {},
   "source": [
    "***Now, let's get started!***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc513191",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "**How it works**:\n",
    "\n",
    "In linear regression, the relationship between our predictor $x$ and our response variable $y$ was:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x$\n",
    "\n",
    "If our $y$ values are all 0 or 1 (and assumed to be *Bernoulli distributed* with probability $p$), this approach doesn't work very well:\n",
    "1. It predicts values <0 and >1 for some inputs $x$\n",
    "2. It doesn't accomodate the fact that getting closer and closer to 1 gets harder and harder: one-unit changes in $x$ may not have equal changes in $p(y = 1)$. \n",
    "\n",
    "*So what do we do about this?*\n",
    "\n",
    "Instead, we postulate the following relationship between $x$ and $y$:\n",
    "\n",
    "$log \\dfrac{p(y=1)}{p(y=0)} = \\beta_0 + \\beta_1 x$.\n",
    "\n",
    "Every unit increase in $x$ leads to a $\\beta_1$ increase in the *log odds of $y$* (or, every unit increase in $x$ leads to a $\\beta_1$ *multiplication* of the *odds* of $y$).\n",
    "\n",
    "This *logit transform* of our response variable $y$ solves both of the problems with linear regression above. \n",
    "\n",
    "However, the goal today isn't to get into the nitty-gritty of logistic regression. Instead, let's talk about how we use it as a classifier!\n",
    "\n",
    "**Classification**\n",
    "\n",
    "When we've fit a logistic regression to our data, we can output a probability $p(y)$ for any given $x$:\n",
    "\n",
    "$p(y) = \\dfrac{e^{h(x)}}{1+ e^{h(x)}}$\n",
    "\n",
    "for $h(x) = \\beta_0 + \\beta_1x$.\n",
    "\n",
    "$\\dfrac{e^{h(x)}}{1+ e^{h(x)}}$ is the *logistic function* that maps from our $x$ variable to $p(y)$.\n",
    "\n",
    "We can use this function as the basis for classification, where $p(y)$ greater than a threshold $T$ is given a particular label estimate $\\hat{y}$.  \n",
    "\n",
    "\n",
    "**Fitting parameters**\n",
    "\n",
    "Even though logistic regression produces regression coefficients (intercept + slopes) similar to linear regression, these parameters are not estimated using the Ordinary Least Squares process we saw with linear regression. Instead, they are most often estimated using a more complicated process called Maximum Likelihood Estimation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c91de",
   "metadata": {},
   "source": [
    "### Logistic regression in python\n",
    "\n",
    "You can read the scikit-learn documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LogisticRegression class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the logistic regression\n",
    "log_reg = LogisticRegression(random_state = 1)\n",
    "\n",
    "# Fit the model\n",
    "log_reg.fit(X = xtrain, y = ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed6011",
   "metadata": {},
   "source": [
    "**What attributes do we get from this model fit?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9aa107",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.classes_\n",
    "\n",
    "log_reg.intercept_ # What does this mean?\n",
    "# np.exp(log_reg.intercept_[0]) / (1 + np.exp(log_reg.intercept_[0]))\n",
    "\n",
    "log_reg.coef_ # What does this mean?\n",
    "# np.exp(log_reg.coef_[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b443b9c",
   "metadata": {},
   "source": [
    "**What functions does the model class give us?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb683feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "binary_preds = log_reg.predict(xtest)\n",
    "binary_preds\n",
    "\n",
    "soft_preds = log_reg.predict_proba(xtest)\n",
    "soft_preds\n",
    "# soft_preds[:, 0] # probability of 0\n",
    "\n",
    "\n",
    "# Accuracy of hard classification predictions\n",
    "log_reg.score(X = xtest, y = ytest) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3a2e9",
   "metadata": {},
   "source": [
    "**How did we do?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show the actual test data\n",
    "g = sns.scatterplot(x = xtest[:, 0], y = ytest, hue = ytest == binary_preds)\n",
    "\n",
    "# Now, let's plot our logistic regression curve\n",
    "sns.lineplot(x = xtest[:, 0], y = soft_preds[:, 1])\n",
    "\n",
    "# What is the \"hard classification\" boundary?\n",
    "sns.lineplot(x = xtest[:, 0], y = binary_preds)\n",
    "plt.axhline(0.5, linestyle = \"--\", color = \"k\") # this is what produces our classification boundary\n",
    "\n",
    "\n",
    "g.set_xlabel(\"Age\")\n",
    "g.set_ylabel(\"CDH probability\")\n",
    "plt.legend(title = \"Correct\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2df95c7",
   "metadata": {},
   "source": [
    "Let's look at where the blue line above comes from.\n",
    "\n",
    "Our logistic regression is formalized as follows:\n",
    "\n",
    "For $h(x) = \\beta_0 + \\beta_1x$,\n",
    "\n",
    "$p(y) = \\dfrac{e^{h(x)}}{1+ e^{h(x)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a633e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement the above transformation here\n",
    "ypreds = np.exp(log_reg.intercept_ + log_reg.coef_*xtest) / (1 + np.exp(log_reg.intercept_ + log_reg.coef_*xtest))\n",
    "\n",
    "# Now we can confirm that this worked\n",
    "g = sns.lineplot(x = xtest[:, 0], y = ypreds[:, 0])\n",
    "g.set_ylim(0, 1)\n",
    "g.set_xlabel(\"Age\")\n",
    "g.set_ylabel(\"p(CDH)\")\n",
    "plt.show()\n",
    "\n",
    "# Finally, let's look at the \"linear\" relationship underlying logistic regression\n",
    "h = sns.lineplot(x = xtest[:, 0], y = np.log(ypreds[:, 0]/(1-ypreds[:, 0])))\n",
    "h.set_xlabel(\"Age\")\n",
    "h.set_ylabel(\"Log odds of CDH\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e97bd",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db6c902",
   "metadata": {},
   "source": [
    "Decision trees are a form of classification that fits a model by generating successive *rules* based on the input feature values. These rules are optimized to try and classify the data as accurately as possible.\n",
    "\n",
    "![decision_tree](img/Decision_Tree.jpeg)\n",
    "\n",
    "Above, the percentages are the percent of data points in each node and the proportions are the probability of survival ([Source](https://en.wikipedia.org/wiki/Decision_tree_learning)).\n",
    "\n",
    "*Take a second to interpret this*.\n",
    "\n",
    "Decision trees have the advantage of being super intuitive (like $k$-nearest neighbors, they're similar to how people often think about classification). \n",
    "\n",
    "There's a great article about how they work [here](https://towardsdatascience.com/decision-tree-classifier-explained-in-real-life-picking-a-vacation-destination-6226b2b60575) and a nice explanation of how the decision boundaries are identified [here](https://victorzhou.com/blog/gini-impurity/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4bf91",
   "metadata": {},
   "source": [
    "### Decision tree classifiers in python\n",
    "\n",
    "You can read the decision tree classifier documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DecisionTreeClassifier class\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Initialize the decision tree classifier\n",
    "dtree = DecisionTreeClassifier(random_state = 1)\n",
    "\n",
    "# Fit the model\n",
    "dtree.fit(X = xtrain, y = ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f93b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree.plot_tree(dtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61467d61",
   "metadata": {},
   "source": [
    "Whoa. \n",
    "\n",
    "**Decision trees can overfit data *a lot* if they aren't constrained.**\n",
    "\n",
    "Let's try this again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45577bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(\n",
    "    max_depth = 1,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "dtree.fit(X = xtrain, y = ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085045f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(dtree,\n",
    "               feature_names = ['Age'],\n",
    "               class_names = ['No CDH', 'CDH'],\n",
    "               filled = True\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6195648",
   "metadata": {},
   "source": [
    "***What's going on here?***\n",
    "\n",
    "- `Age <= 50.5`: This is the \"rule\" being used to define leaves on either side of the tree (\"No\" -> left, \"Yes\" -> right)\n",
    "- `gini = 0.453`: This refers to the \"Gini impurity\" of the node. Gini impurity is the loss function used to fit this tree (optimal = 0) (more on this [here](https://victorzhou.com/blog/gini-impurity/))\n",
    "- `samples = 346`: This is the number of samples in the group that the node is dividing\n",
    "- `value = [226, 120]`: This is the number of training values on the left (`values[0]`) and the right (`values[1]`) of the node\n",
    "\n",
    "NOTE: With a depth of 1, at the very bottom, we have:\n",
    "- 170 people were correctly classified as \"No CDH\" with this rule (true negatives)\n",
    "- 47 people were classified as \"No CDH\" with this rule *incorrectly* (false negatives)\n",
    "- 56 people were classified as \"CDH\" with this rule *incorrectly* (false positives)\n",
    "- 73 people were classified as \"CDH\" with this rule *correctly* (true positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dcdffc",
   "metadata": {},
   "source": [
    "Like other classifiers, the decision tree classifier lets us predict values and has functions for assessing prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8552518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on the data\n",
    "dtree.score(X = xtrain, y = ytrain)\n",
    "dtree.score(X = xtest, y = ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f3ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypreds = dtree.predict(X = xtest)\n",
    "ypreds\n",
    "\n",
    "# Test \"score\" above\n",
    "sum(ypreds == ytest) / len(ypreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"soft classification\" probabilities are just the fraction of training samples for the \"true\" label \n",
    "# in the leaf where this test item ended up\n",
    "\n",
    "ypreds_soft = dtree.predict_proba(X = xtest)\n",
    "ypreds_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd0f65",
   "metadata": {},
   "source": [
    "We can use the predictions as the basis for betting understanding what the tree is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4326b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reveals the cutoff(s) chosen by our decision tree! \n",
    "train_preds = dtree.predict(X = xtrain)\n",
    "g = sns.scatterplot(x = xtrain[:, 0], y = ytrain, hue = ytrain == train_preds)\n",
    "g.axvline(50.5)\n",
    "# g.axvline(59.5)\n",
    "# g.axvline(24.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6534b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "# Make a similar graph to the above with the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3711d02",
   "metadata": {},
   "source": [
    "We can also draw on the same resources that we talked about for assessing our $k$-nearest neighbors classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc94139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Test accuracy\n",
    "accuracy_score(y_true = ytest, y_pred = dtree.predict(X = xtest))\n",
    "\n",
    "# Test F1 score\n",
    "f1_score(y_true = ytest,\n",
    "         y_pred = dtree.predict(X = xtest),\n",
    "         labels = [0, 1],\n",
    "         pos_label = 1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddab997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(\n",
    "    y_true = ytest,\n",
    "    y_score = dtree.predict_proba(X = xtest)[:, 1],\n",
    "    pos_label = 1\n",
    ")\n",
    "\n",
    "\n",
    "sns.lineplot(x = fpr, y = tpr)\n",
    "plt.axline(xy1 = (0, 0), slope = 1, c = \"r\")\n",
    "\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168ad36a",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVMs)\n",
    "\n",
    "Support Vector Machines work by trying to find a line or plane (usually in a high-dimensional space) that *maximally separates* the training labels in that space. \n",
    "\n",
    "The intuition for this is relatively straightforward but the implementation can get complicated!\n",
    "\n",
    "In the plot below, the linear funtion $h_3(x_1, x_2)$ is the best way to separate our training data because it maximizes the margin on either side of the line.\n",
    "\n",
    "SVMs try to find the equivalent of $h_3$ given some training data. This separator can be defined by the closest points in the data to the line; these are called the \"support vectors\". Finding the best separator usually requires mapping the training data into a high-dimensional space where it can be effectively separated.\n",
    "\n",
    "![svm](img/svm2.png)\n",
    "\n",
    "([Source](https://en.wikipedia.org/wiki/Support-vector_machine))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfdf8c9",
   "metadata": {},
   "source": [
    "### SVMs in python\n",
    "\n",
    "The documentation for SVMs in scikit-learn is [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54028157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "\n",
    "svm.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b7557",
   "metadata": {},
   "source": [
    "In the case of SVMs, there are class attributes that help you recover the separator that was fit.\n",
    "\n",
    "We won't get into these but if you're interested in learning more it's good to know about!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8d0e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm.intercept_\n",
    "# svm.coef_ # only for 'linear' kernel\n",
    "# svm.support_vectors_\n",
    "\n",
    "# For example, we can view the items in the training set that formed the support vector\n",
    "sns.scatterplot(x = xtrain[:, 0], y = ytrain)\n",
    "plt.title(\"Training data\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x = xtrain[svm.support_][:, 0], y = ytrain[svm.support_])\n",
    "plt.title(\"Support vectors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fe1a39",
   "metadata": {},
   "source": [
    "The SVM class has a `score` function that returns the accuracy of a test set, plus prediction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c351d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent of correct classifications\n",
    "svm.score(X = xtrain, y = ytrain)\n",
    "svm.score(X = xtest, y = ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f060c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypreds = svm.predict(X = xtest)\n",
    "ypreds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d509b07",
   "metadata": {},
   "source": [
    "However, soft prediction requires configuring the initial model to do soft classification (by default, SVMs are made to only do hard classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ce902",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_soft = SVC(probability = True) # indicate that you want the SVM to do soft classification\n",
    "svm_soft.fit(X = xtrain, y = ytrain)\n",
    "\n",
    "ypreds_soft = svm_soft.predict_proba(X = xtest)\n",
    "ypreds_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf8dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4567a38",
   "metadata": {},
   "source": [
    "# Classifier Wrap-Up\n",
    "\n",
    "This is just a sample of what's out there!\n",
    "\n",
    "There are a number of other common classifiers you should take a look at if you're interested:\n",
    "- Naive Bayes ([here](https://scikit-learn.org/stable/modules/naive_bayes.html))\n",
    "- Discriminant analysis ([linear](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) and [quadratic](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html))\n",
    "- Neural networks ([here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html))\n",
    "- Random forests ([here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)) (related to decision trees)\n",
    "- Gradient boosted trees ([here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html))\n",
    "- ...\n",
    "\n",
    "The main goal of this lecture is to show you some of the creative ways that people solve classification problems and how the scikit-learn library supports these solutions. \n",
    "\n",
    "This should empower you to go off and try some of these other ones on your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac22d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e774977668b7c0ae8309835a5187aa7fbf7669e7d0bb59755bc63e573643edcd"
  },
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
